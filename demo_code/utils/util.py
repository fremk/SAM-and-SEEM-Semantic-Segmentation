# adopted from
# https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py
# and
# https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py
# and
# https://github.com/openai/guided-diffusion/blob/0ba878e517b276c45d1195eb29f6f5f72659a05b/guided_diffusion/nn.py
#
# thanks!
import importlib
import matplotlib 
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import os
import math
import torch
import torch.nn as nn
import numpy as np
from einops import repeat
import cv2
import numpy as np
from utils.arguments import load_opt_from_config_files
from utils.distributed import init_distributed
from xdecoder.BaseModel import BaseModel
from xdecoder import build_model
from utils.constants import COCO_PANOPTIC_CLASSES, classes_cmap,cmap
import whisper
import matplotlib.patches as mpatches

import argparse
from PIL import Image
from tasks.interactive import interactive_infer_image

'''
build args
'''
opt = load_opt_from_config_files("configs/seem/seem_focall_lang.yaml")
opt = init_distributed(opt)

cur_model = 'None'
pretrained_pth = os.path.join("seem_focall_v1.pt")
cur_model = 'Focal-L'

'''
build model
'''
model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()
with torch.no_grad():
    model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(COCO_PANOPTIC_CLASSES + ["background"], is_eval=True)

'''
audio
'''
audio = whisper.load_model("base")
@torch.no_grad()

def inference(image, task, *args, **kwargs):
    with torch.autocast(device_type='cuda', dtype=torch.float16):
        return interactive_infer_image(model, audio, image, task, *args, **kwargs)


def draw_segmentation(img,a=1,cmap=cmap,classes_cmap=classes_cmap):
    arrayShow = np.array([[np.array(cmap[i])/255.0 for i in j] for j in img]) 
    patches =[mpatches.Patch(color=np.array(cmap[i])/255.0,label=classes_cmap[i]) for i in np.unique(img)]
    # put those patched as legend-handles into the legend
    plt.imshow(arrayShow,alpha=a)
    plt.legend(loc='center left', bbox_to_anchor=(1.01, 0.5),handles=patches, borderaxespad=0.)
    plt.show()

def divide_list_into_k_lists(lst, k):

    if (k == 0 or k == 1):
        return [lst]

    divided_lists = []
    for i in range(k):
        divided_lists.append(lst[i*len(images)//k:(i+1)*len(images)//k])

    return divided_lists



def sam_inference(image,mask_generator):
    """
    Perform Semantic Annotation and Mask (SAM) inference on an input image.

    This function generates and classifies masks based on the input image using SAM.

    Parameters:
    - image (numpy.ndarray): The input image of size (W,H,3) on which SAM inference will be performed. 
    - mask_generator: An predefined SAM mask generator that generates masks based on the input image.

    Returns:
    - numpy.ndarray: A (W,H,1) image with segmented regions. Each region corresponds to an integer value. 
    The unsegmented region is set to value 30000.

    SAM Inference Steps:
    1. Generate masks using the provided SAM mask generator.
    2. Filter the generated masks by removing overlapping or redundant masks.
    3. Combine the filtered masks into a single mask image.
    4. Fill in the remaining non-segmented areas if necessary.
    """
    kernel3 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2 * 10 + 1, 2 * 10 + 1))
    masks = mask_generator.generate(image)
    #----------------------------------------------------------------------------------------------

    # Filtering the masks
    newlist = sorted(masks, key=lambda d: d['area'])
    for i in range(len(newlist)-1,-1,-1):
        for j in range(i):
            if(i<len(newlist) and j <len(newlist)):
                overlap=newlist[i]['segmentation']*newlist[j]['segmentation']
                if(overlap.sum()>0):
                    newlist[i]['segmentation']=newlist[i]['segmentation']^overlap
                # if(overlap.sum()==newlist[j]['area']):
                #     del newlist[j]
                # elif(overlap.sum()>0 and overlap.sum()<newlist[j]['area']):
                #     newlist[j]['segmentation']=newlist[j]['segmentation']^overlap

    if(len(newlist)<10):
        newlist=sorted(masks, key=lambda d: d['area'])
        for i in range(len(newlist)-1,-1,-1):
            for j in range(i):
                if(i<len(newlist) and j <len(newlist)):
                    overlap=newlist[i]['segmentation']*newlist[j]['segmentation']
                    if(overlap.sum()>0):
                        newlist[i]['segmentation']=newlist[i]['segmentation']^overlap

    print('number of masks generated by SAM:' + str(len(newlist)))

    # Adding all the masks together into a single mask
    for i in range(len(newlist)):
        if i==0:
            mask=newlist[i]['segmentation'].astype('uint32')*(i+1)
        else:
            mask=mask+newlist[i]['segmentation']*(i+1)
    
    #-------------------------------------------------------------------------------------------------------
    gap_mask = np.where(mask==0,1,0)

    ratio=np.sum(gap_mask)/(gap_mask.shape[0]*gap_mask.shape[1])
    print('Percentage of the image that is not segmented '+ str(ratio)[:7])

    # Filling in the remaining non segmented areas

    # If majority of image is not segmented, perform opening then separate the remaining masks and add them to the list of masks
    # if ratio > 0.05:
        
        # opened_array = cv2.morphologyEx(gap_mask.astype('uint8'), cv2.MORPH_OPEN, kernel3)
        
        # # Getting each mask seperately
        # num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(opened_array)

        # labels[labels!=0]=labels[labels!=0]+np.amax(mask)

        # mask=mask+labels

    # Else if majority of the image is already segmented, this leaves the borders which are directly filled using KNN
    
    # elif(ratio==0):
        # pass
    # else:
        # pass

    print('\n------------------------------------------------')
    print('Masks generation done, you can now classify them using SEEM')
    print('------------------------------------------------\n')

    mask[mask==0]=30000
    return mask

def seem_inference(image,mask):
    """
    Perform SEEM inference on the input image aswell as the SAM generated masks.

    This function classifies and refines the segmented masks using SEEM.

    Parameters:
    - image (numpy.ndarray): The input image on which SEEM inference will be performed.
    - mask (numpy.ndarray): The SAM generated masks in a single array where each value in that np array corresponds to a mask.

    Returns:
    - result_mask (PIL.Image): Final semantic segmentation output using the full pipeline.
    - conf (numpy.ndarray): SEEM generated logits (W,H,133) to be able to generate the confidence scores from them.
    - seg (numpy.ndarray): Final semantic segmentation output using only SEEM semantic segmentation.

    SEEM Inference Steps:
    1. Initialize morphological kernels for image processing.
    2. Use the provided mask for segmentation.
    3. Perform SEEM semantic segmentation on the input image.
    4. Classify and refine the segmented masks using SEEM.
    5. Create the final result_mask with semantic class labels.
    """

    kernel1 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2 * 5 + 1, 2 * 5 + 1))
    kernel2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2 * 1 + 1, 2 * 1 + 1))

    masks=mask

    out_mask=np.zeros(np.array(masks).shape)

    im = Image.fromarray(image)
    seg,conf=inference(im,"Panoptic")


    for i in np.unique(masks):
        if i!=30000:
            mask=np.where(np.array(masks)==i,1,0)
            eroded_mask = cv2.erode(mask.astype('uint8'), kernel1, iterations=1)
            eroded_mask=np.where(eroded_mask==1,True,False)
            if(np.sum(eroded_mask)==0):
                eroded_mask =np.where(mask==1,True,False)
            elif(np.sum(eroded_mask)/np.sum(mask)<0.3):
                eroded_mask = cv2.erode(mask.astype('uint8'), kernel2, iterations=1)
                eroded_mask = np.where(eroded_mask==1,True,False)
                

            # print(mask.shape)
            confidence , c=inference(im,"Stroke",mask=eroded_mask)
            if(c==255):
                masked_pan=np.where(mask==1,seg,90)
                colors, color_count=np.unique(masked_pan,return_counts=True)

                color_count=np.delete(color_count,np.where(colors==90))
                colors=np.delete(colors,np.where(colors==90))

                arr1inds = color_count.argsort()
                color_count = color_count[arr1inds[::-1]]
                colors = colors[arr1inds[::-1]]

                if len(colors)==1:
                    final_class=colors[0]
                else:
                    if(colors[0] == 255):
                        if(color_count[1]/color_count[0]>0.5):
                            final_class=colors[1]
                        else:
                            final_class=colors[0]
                    else:
                        final_class=colors[0]


                out_mask=out_mask+final_class*mask
                # print('------------------------------------')

            else:
                out_mask=out_mask+c*mask
        else:
            mask=np.where(np.array(masks)==i,1,0)
            out_mask=out_mask+255*mask

    mask=np.where(out_mask==255,True,False)

    npseg=np.array(seg)
    out_mask[out_mask==0]=200
    out_mask[out_mask==255]=0
    out_mask=out_mask+mask*np.array(seg)
    out_mask[out_mask==200]=0

    # out_mask[npseg==8]=8
    # out_mask[npseg==12]=12


    if(np.amax(np.unique(out_mask))>27 and np.amax(np.unique(out_mask))!=255):
        print('\n-------------------------------------')
        print("YOU HAVE OVERLAPPING MASKS! SOMETHING WRONG WITH THE SEGMENTATION")
        print('-------------------------------------\n')

    result_mask = Image.fromarray(out_mask.astype(np.uint32)).convert('L')

    return result_mask,conf,seg


def final_mapping(image):
    """
    Perform mapping of semantic class labels in an input image.

    This function maps the semantic class labels to another set of labels in which some classes have been merged and others have been removed.

    Parameters:
    - image (numpy.ndarray): The input semantic segmentation of an image with 28 classes.

    Returns:
    - mapped_image (numpy.ndarray): The output semantic segmentation of the same image but with 19 classes.
    """

    mapping = {
        1: 0, 16: 0, 20: 0,
        2: 1, 3: 2, 6: 2, 9: 2, 10: 2,
        4: 3, 5: 4, 7: 5, 8: 6,
        11: 7, 12: 8, 13: 9, 14: 10, 15: 11,
        17: 317, 18: 12, 19: 13,
        21: 321, 22: 18, 23: 14, 24: 15, 25: 16, 26: 17, 27: 18,
        321: 18, 61: 18, 65: 18, 317: 18
    }
    
    im = image.copy().astype(np.uint32)
    
    for k, v in mapping.items():
        im[im == k] = v

    return im
def instantiate_from_config(config):
    if not "target" in config:
        if config == '__is_first_stage__':
            return None
        elif config == "__is_unconditional__":
            return None
        raise KeyError("Expected key `target` to instantiate.")
    return get_obj_from_str(config["target"])(**config.get("params", dict()))


def get_obj_from_str(string, reload=False):
    module, cls = string.rsplit(".", 1)
    if reload:
        module_imp = importlib.import_module(module)
        importlib.reload(module_imp)
    return getattr(importlib.import_module(module, package=None), cls)


def make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):
    if schedule == "linear":
        betas = (
                torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2
        )

    elif schedule == "cosine":
        timesteps = (
                torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s
        )
        alphas = timesteps / (1 + cosine_s) * np.pi / 2
        alphas = torch.cos(alphas).pow(2)
        alphas = alphas / alphas[0]
        betas = 1 - alphas[1:] / alphas[:-1]
        betas = np.clip(betas, a_min=0, a_max=0.999)

    elif schedule == "sqrt_linear":
        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)
    elif schedule == "sqrt":
        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64) ** 0.5
    else:
        raise ValueError(f"schedule '{schedule}' unknown.")
    return betas.numpy()


def make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):
    if ddim_discr_method == 'uniform':
        c = num_ddpm_timesteps // num_ddim_timesteps
        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))
    elif ddim_discr_method == 'quad':
        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)
    else:
        raise NotImplementedError(f'There is no ddim discretization method called "{ddim_discr_method}"')

    # assert ddim_timesteps.shape[0] == num_ddim_timesteps
    # add one to get the final alpha values right (the ones from first scale to data during sampling)
    steps_out = ddim_timesteps + 1
    if verbose:
        print(f'Selected timesteps for ddim sampler: {steps_out}')
    return steps_out


def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):
    # select alphas for computing the variance schedule
    alphas = alphacums[ddim_timesteps]
    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())

    # according the the formula provided in https://arxiv.org/abs/2010.02502
    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))
    if verbose:
        print(f'Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}')
        print(f'For the chosen value of eta, which is {eta}, '
              f'this results in the following sigma_t schedule for ddim sampler {sigmas}')
    return sigmas, alphas, alphas_prev


def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):
    """
    Create a beta schedule that discretizes the given alpha_t_bar function,
    which defines the cumulative product of (1-beta) over time from t = [0,1].
    :param num_diffusion_timesteps: the number of betas to produce.
    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and
                      produces the cumulative product of (1-beta) up to that
                      part of the diffusion process.
    :param max_beta: the maximum beta to use; use values lower than 1 to
                     prevent singularities.
    """
    betas = []
    for i in range(num_diffusion_timesteps):
        t1 = i / num_diffusion_timesteps
        t2 = (i + 1) / num_diffusion_timesteps
        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))
    return np.array(betas)


def extract_into_tensor(a, t, x_shape):
    b, *_ = t.shape
    out = a.gather(-1, t)
    return out.reshape(b, *((1,) * (len(x_shape) - 1)))


def checkpoint(func, inputs, params, flag):
    """
    Evaluate a function without caching intermediate activations, allowing for
    reduced memory at the expense of extra compute in the backward pass.
    :param func: the function to evaluate.
    :param inputs: the argument sequence to pass to `func`.
    :param params: a sequence of parameters `func` depends on but does not
                   explicitly take as arguments.
    :param flag: if False, disable gradient checkpointing.
    """
    if flag:
        args = tuple(inputs) + tuple(params)
        return CheckpointFunction.apply(func, len(inputs), *args)
    else:
        return func(*inputs)


class CheckpointFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, run_function, length, *args):
        ctx.run_function = run_function
        ctx.input_tensors = list(args[:length])
        ctx.input_params = list(args[length:])

        with torch.no_grad():
            output_tensors = ctx.run_function(*ctx.input_tensors)
        return output_tensors

    @staticmethod
    def backward(ctx, *output_grads):
        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]
        with torch.enable_grad():
            # Fixes a bug where the first op in run_function modifies the
            # Tensor storage in place, which is not allowed for detach()'d
            # Tensors.
            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]
            output_tensors = ctx.run_function(*shallow_copies)
        input_grads = torch.autograd.grad(
            output_tensors,
            ctx.input_tensors + ctx.input_params,
            output_grads,
            allow_unused=True,
        )
        del ctx.input_tensors
        del ctx.input_params
        del output_tensors
        return (None, None) + input_grads


def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):
    """
    Create sinusoidal timestep embeddings.
    :param timesteps: a 1-D Tensor of N indices, one per batch element.
                      These may be fractional.
    :param dim: the dimension of the output.
    :param max_period: controls the minimum frequency of the embeddings.
    :return: an [N x dim] Tensor of positional embeddings.
    """
    if not repeat_only:
        half = dim // 2
        freqs = torch.exp(
            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half
        ).to(device=timesteps.device)
        args = timesteps[:, None].float() * freqs[None]
        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
        if dim % 2:
            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
    else:
        embedding = repeat(timesteps, 'b -> b d', d=dim)
    return embedding


def zero_module(module):
    """
    Zero out the parameters of a module and return it.
    """
    for p in module.parameters():
        p.detach().zero_()
    return module


def scale_module(module, scale):
    """
    Scale the parameters of a module and return it.
    """
    for p in module.parameters():
        p.detach().mul_(scale)
    return module


def mean_flat(tensor):
    """
    Take the mean over all non-batch dimensions.
    """
    return tensor.mean(dim=list(range(1, len(tensor.shape))))


def normalization(channels):
    """
    Make a standard normalization layer.
    :param channels: number of input channels.
    :return: an nn.Module for normalization.
    """
    return GroupNorm32(32, channels)


# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.
class SiLU(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)


class GroupNorm32(nn.GroupNorm):
    def forward(self, x):
        return super().forward(x.float()).type(x.dtype)

def conv_nd(dims, *args, **kwargs):
    """
    Create a 1D, 2D, or 3D convolution module.
    """
    if dims == 1:
        return nn.Conv1d(*args, **kwargs)
    elif dims == 2:
        return nn.Conv2d(*args, **kwargs)
    elif dims == 3:
        return nn.Conv3d(*args, **kwargs)
    raise ValueError(f"unsupported dimensions: {dims}")


def linear(*args, **kwargs):
    """
    Create a linear module.
    """
    return nn.Linear(*args, **kwargs)


def avg_pool_nd(dims, *args, **kwargs):
    """
    Create a 1D, 2D, or 3D average pooling module.
    """
    if dims == 1:
        return nn.AvgPool1d(*args, **kwargs)
    elif dims == 2:
        return nn.AvgPool2d(*args, **kwargs)
    elif dims == 3:
        return nn.AvgPool3d(*args, **kwargs)
    raise ValueError(f"unsupported dimensions: {dims}")


class HybridConditioner(nn.Module):

    def __init__(self, c_concat_config, c_crossattn_config):
        super().__init__()
        self.concat_conditioner = instantiate_from_config(c_concat_config)
        self.crossattn_conditioner = instantiate_from_config(c_crossattn_config)

    def forward(self, c_concat, c_crossattn):
        c_concat = self.concat_conditioner(c_concat)
        c_crossattn = self.crossattn_conditioner(c_crossattn)
        return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}


def noise_like(shape, device, repeat=False):
    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))
    noise = lambda: torch.randn(shape, device=device)
    return repeat_noise() if repeat else noise()


